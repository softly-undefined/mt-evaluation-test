{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU and COMET Exploration and Basic Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "from comet import download_model, load_from_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to explore the CWMT datasets I found on Kaggle [here](https://www.kaggle.com/datasets/warmth/cwmt-data). \n",
    "\n",
    "The dataset contains a large quantity of information from several years of CWMT conferences (2008, 2009, 2011) as well as a number of other sources, though for the purposes of this exploration I will be limiting my observations to specificially Chinese -> English datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datasource</th>\n",
       "      <th>domain</th>\n",
       "      <th>setid</th>\n",
       "      <th>srclang</th>\n",
       "      <th>trglang</th>\n",
       "      <th>src</th>\n",
       "      <th>ref1</th>\n",
       "      <th>ref2</th>\n",
       "      <th>ref3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cwmt2008</td>\n",
       "      <td>ce-news</td>\n",
       "      <td>zh_en_news_trans</td>\n",
       "      <td>zh</td>\n",
       "      <td>en</td>\n",
       "      <td>狭小的防震棚已经成为北川擂鼓镇农民张秀华（58岁）临时的家，而就在这个“家”的中央，悬挂了一...</td>\n",
       "      <td>A small narrow anti-earthquake tent became the...</td>\n",
       "      <td>The shockproof shed has become a temporary hom...</td>\n",
       "      <td>The narrow quakeproof shelter has become the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cwmt2008</td>\n",
       "      <td>ce-news</td>\n",
       "      <td>zh_en_news_trans</td>\n",
       "      <td>zh</td>\n",
       "      <td>en</td>\n",
       "      <td>画像中，中共中央总书记胡锦涛和国务院总理温家宝两人在绵阳机场紧紧握手，画像下有一行题字：“伟...</td>\n",
       "      <td>In this portrait, Hu Jintao, the General Secre...</td>\n",
       "      <td>The picture showed General Secretary of the Co...</td>\n",
       "      <td>Hu Jintao, the general secretary of the CPC Ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cwmt2008</td>\n",
       "      <td>ce-news</td>\n",
       "      <td>zh_en_news_trans</td>\n",
       "      <td>zh</td>\n",
       "      <td>en</td>\n",
       "      <td>5月16日，四川汶川大地震发生后的第四天，胡锦涛从北京飞抵四川绵竹机场，亲自指挥抗震救灾。</td>\n",
       "      <td>On May 16th, four days after the Wenchuan eart...</td>\n",
       "      <td>On May 16, the fourth day after the Wenchuan E...</td>\n",
       "      <td>On May 16, the 4th day following Sichuan Wench...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cwmt2008</td>\n",
       "      <td>ce-news</td>\n",
       "      <td>zh_en_news_trans</td>\n",
       "      <td>zh</td>\n",
       "      <td>en</td>\n",
       "      <td>地震后当天就飞到灾区指挥的温家宝到机场迎接，两人一见面，就在飞机前握手致意。</td>\n",
       "      <td>Wen Jiabao, who flew to the disaster area same...</td>\n",
       "      <td>Wen Jiabao, who has arrived at the disaster ar...</td>\n",
       "      <td>Wen Jiabao, who flew to the quake-hit areas an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cwmt2008</td>\n",
       "      <td>ce-news</td>\n",
       "      <td>zh_en_news_trans</td>\n",
       "      <td>zh</td>\n",
       "      <td>en</td>\n",
       "      <td>张秀华家挂的胡、温画像是经过电脑处理，原来画面的其他人员已经被掩盖，只有两个人握手的画面。</td>\n",
       "      <td>The portrait of Hu and Wen hung in Zhang Xiuhu...</td>\n",
       "      <td>The portrait of Hu and Wen hung in Zhang Xiuhu...</td>\n",
       "      <td>The figure of President Hu and Premier Wen hun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datasource   domain             setid srclang trglang   \n",
       "0   cwmt2008  ce-news  zh_en_news_trans      zh      en  \\\n",
       "1   cwmt2008  ce-news  zh_en_news_trans      zh      en   \n",
       "2   cwmt2008  ce-news  zh_en_news_trans      zh      en   \n",
       "3   cwmt2008  ce-news  zh_en_news_trans      zh      en   \n",
       "4   cwmt2008  ce-news  zh_en_news_trans      zh      en   \n",
       "\n",
       "                                                 src   \n",
       "0  狭小的防震棚已经成为北川擂鼓镇农民张秀华（58岁）临时的家，而就在这个“家”的中央，悬挂了一...  \\\n",
       "1  画像中，中共中央总书记胡锦涛和国务院总理温家宝两人在绵阳机场紧紧握手，画像下有一行题字：“伟...   \n",
       "2      5月16日，四川汶川大地震发生后的第四天，胡锦涛从北京飞抵四川绵竹机场，亲自指挥抗震救灾。   \n",
       "3             地震后当天就飞到灾区指挥的温家宝到机场迎接，两人一见面，就在飞机前握手致意。   \n",
       "4      张秀华家挂的胡、温画像是经过电脑处理，原来画面的其他人员已经被掩盖，只有两个人握手的画面。   \n",
       "\n",
       "                                                ref1   \n",
       "0  A small narrow anti-earthquake tent became the...  \\\n",
       "1  In this portrait, Hu Jintao, the General Secre...   \n",
       "2  On May 16th, four days after the Wenchuan eart...   \n",
       "3  Wen Jiabao, who flew to the disaster area same...   \n",
       "4  The portrait of Hu and Wen hung in Zhang Xiuhu...   \n",
       "\n",
       "                                                ref2   \n",
       "0  The shockproof shed has become a temporary hom...  \\\n",
       "1  The picture showed General Secretary of the Co...   \n",
       "2  On May 16, the fourth day after the Wenchuan E...   \n",
       "3  Wen Jiabao, who has arrived at the disaster ar...   \n",
       "4  The portrait of Hu and Wen hung in Zhang Xiuhu...   \n",
       "\n",
       "                                                ref3  \n",
       "0  The narrow quakeproof shelter has become the t...  \n",
       "1  Hu Jintao, the general secretary of the CPC Ce...  \n",
       "2  On May 16, the 4th day following Sichuan Wench...  \n",
       "3  Wen Jiabao, who flew to the quake-hit areas an...  \n",
       "4  The figure of President Hu and Premier Wen hun...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the datasets properly for manipulation\n",
    "df2008 = pd.read_csv(\"mt-dataset/cwmt2008_ce_news.tsv\", delimiter=\"\\t\")\n",
    "df2009 = pd.read_csv(\"mt-dataset/cwmt2009_ce_news.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "# drops the 4 rows between the two datasets missing a third reference\n",
    "df2008.dropna()\n",
    "df2009.dropna()\n",
    "\n",
    "df2008.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two datasets I will be using are now properly imported, note that each source Chinese code has not one but three \"correct\" English reference translations.\n",
    "\n",
    "My goal with this project is to learn about the application of the BLEU and COMET MT evaluation metrics, and to do this I will be evaluating the two main LLMs I've been using for my [Classical Chinese machine translation interface](https://github.com/softly-undefined/classical-chinese-tool-v2) off of two baseline scores:\n",
    "\n",
    "1. A translation completed by Google Translate, a commonly accepted machine translation tool used widely\n",
    "2. An approved reference translation, calculating BLEU and COMET comparing ref1 as the MT-generated output to ref2 and ref3 as references (this may be up for change, I'm not the hugest fan of using a different amount of reference translations for this section compared to earlier ones)\n",
    "3. Potentially Apple Translate, to compare it's efficacy as well, although that is also potentially up for change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this [tutorial](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/) to learn about calculating BLEU scores.\n",
    "\n",
    "\n",
    "COMET- calculates sentence-by-sentence, when looking corpus-wide it is simply an average of the sentence level scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#example code to reference\n",
    "references = [[['this', 'is', 'a', 'test'], ['this', 'is', 'test']]]\n",
    "candidates = [['this', 'is', 'a', 'test']]\n",
    "score = corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Before Generating Translations\n",
    "\n",
    "Later on in this project I will be generating a large number of translations using the different models I have been testing, but I want to ensure that I properly understand how to use the evaluation metrics before spending the time/money creating the translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Score Testing\n",
    "First I will use BLEU to generate scores for the ref1 columns of the 2008 data compared to the ref2 and ref3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2520605875656664\n"
     ]
    }
   ],
   "source": [
    "#extract the relevant data\n",
    "df_test = df2008\n",
    "\n",
    "# format references from ref2 and ref3 columns\n",
    "df_test[['ref2', 'ref3']] = df_test[['ref2', 'ref3']].astype(str)\n",
    "references_test = df_test[['ref2', 'ref3']].values.tolist()\n",
    "references_test = [[sentence.split() for sentence in ref_group] for ref_group in references_test]\n",
    "\n",
    "# format candidates from ref1 column\n",
    "df_test['ref1'] = df_test['ref1'].astype(str)\n",
    "candidates_test = df_test['ref1'].values.tolist()\n",
    "candidates_test = [sentence.split() for sentence in candidates_test]\n",
    "\n",
    "bleu_score = corpus_bleu(references_test, candidates_test)\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I repeated the process using ref2 and ref3 as the candidates resulting in values of 0.2377663462841575 and 0.2208567008054941 respectively. These scores seem to make sense comparing them to numbers from the [original BLEU paper](https://aclanthology.org/P02-1040.pdf)\n",
    "\n",
    "\n",
    "\n",
    "With the knowledge from the [original BLEU paper](https://aclanthology.org/P02-1040.pdf) it is clear that I should not compare BLEU scores based on different numbers of reference translations, so going forward I won't be using these translations to compare (though I may try testing my MT output on only 2 references to make it viable for comparison)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMET Score Testing\n",
    "\n",
    "Next I will test the COMET Metric using the same idea of using the 2008 data and doing ref1 compared to ref2 and ref3\n",
    "\n",
    "Rather than having to split up the sentences of reference and candidate word by word (which BLEU requires) COMET requires a specific format working with the sentences themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting the data correctly using ref1 as candidate and ref2 and ref4 as references\n",
    "formatted_data = []\n",
    "\n",
    "for _, row in df2008.iterrows():\n",
    "    entry = {\n",
    "        \"src\": row['src'],\n",
    "        \"mt\": row['ref1'],\n",
    "        \"ref\": [row['ref2'], row['ref3']]\n",
    "    }\n",
    "    formatted_data.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d034a0e74e114b8e87ff139793e4eec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/ericbennett/miniforge3/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ericbennett/miniforge3/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|████████████████| 126/126 [02:39<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "\n",
    "model = load_from_checkpoint(model_path)\n",
    "\n",
    "model_output = model.predict(formatted_data, batch_size=8, gpus=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7343478642448517\n"
     ]
    }
   ],
   "source": [
    "comet_score = sum(model_output.scores) / len(model_output.scores)\n",
    "print(comet_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ref2 as the candidate resulted in a comet_score of 0.7299891123122296, and ref3 a comet_score of 0.7139009555753609. It's interesting that both BLEU and COMET have a decreasing score order when using different references, probably just chance that it is decreasing but could show the correlation between BLEU and COMET scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Generation\n",
    "\n",
    "I won't be generating the sentences themselves in this notebook file, but I have in separate files within this project, storing the resulting data in two .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations2008 = pd.read_csv(\"mtranslations/translations2008.csv\")\n",
    "translations2009 = pd.read_csv(\"mtranslations/translations2009.csv\")\n",
    "\n",
    "# next here add each of the .csv's to the df2008 and df2009 dataframes respectively\n",
    "\n",
    "\n",
    "# df2008\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model names will be stored in this array for reference\n",
    "# when calculating BLEU and COMET scores \n",
    "translation_models = ['ref1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU Score Calculation\n",
    "\n",
    "Now that we have the translations we are looking to evaluated in our dataframe, we can calculate the BLEU scores for each of our different translation methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2008 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref1 BLEU score 2008 Dataset: 0.9995760206964518\n"
     ]
    }
   ],
   "source": [
    "#creating a references array for the 2008 dataset in a format acceptable to corpus_bleu\n",
    "\n",
    "df2008[['ref1', 'ref2', 'ref3']] = df2008[['ref1', 'ref2', 'ref3']].astype(str)\n",
    "references2008 = df2008[['ref1', 'ref2', 'ref3']].values.tolist()\n",
    "references2008 = [[sentence.split() for sentence in ref_group] for ref_group in references2008]\n",
    "\n",
    "\n",
    "for model in translation_models:\n",
    "    df2008[model] = df2008[model].astype(str)\n",
    "    candidates2008 = df2008[model].values.tolist()\n",
    "    candidates2008 = [sentence.split() for sentence in candidates2008]\n",
    "    \n",
    "    bleu_score = corpus_bleu(references2008, candidates2008)\n",
    "    print(f\"{model} BLEU score (2008 Dataset): {bleu_score}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2009 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref1 BLEU score (2008 Dataset): 0.9995760206964518\n"
     ]
    }
   ],
   "source": [
    "#creating a references array for the 2009 dataset in a format acceptable to corpus_bleu\n",
    "\n",
    "df2009[['ref1','ref2','ref3']] = df2009[['ref1','ref2','ref3']].astype(str)\n",
    "references2009 = df2009[['ref1','ref2','ref3']].values.tolist()\n",
    "references2009 = [[sentence.split() for sentence in ref_group] for ref_group in references2009]\n",
    "\n",
    "\n",
    "for model in translation_models:\n",
    "    df2009[model] = df2009[model].astype(str)\n",
    "    candidates2009 = df2009[model].values.tolist()\n",
    "    candidates2009 = [sentence.split() for sentence in candidates2009]\n",
    "    \n",
    "    bleu_score = corpus_bleu(references2008, candidates2008)\n",
    "    print(f\"{model} BLEU score (2008 Dataset): {bleu_score}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMET Score Calculation\n",
    "\n",
    "Finally, we will calculate the COMET evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2008 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02083a145b3b4aa0a119b6be646a4a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/ericbennett/miniforge3/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ericbennett/miniforge3/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|████████████████| 126/126 [02:33<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "comet_scores2008 = []\n",
    "\n",
    "\n",
    "for model_name in translation_models:    \n",
    "    formatted_data2008 = []\n",
    "    \n",
    "    for _, row in df2008.iterrows():\n",
    "        entry = {\n",
    "            \"src\": row['src'],\n",
    "            \"mt\": row[model_name],\n",
    "            \"ref\": [row['ref1'],row['ref2'], row['ref3']]\n",
    "        }\n",
    "        formatted_data2008.append(entry)\n",
    "\n",
    "    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "    model = load_from_checkpoint(model_path)\n",
    "    model_output = model.predict(formatted_data, batch_size=8, gpus=0)\n",
    "    \n",
    "    comet_score = sum(model_output.scores) / len(model_output.scores)\n",
    "    \n",
    "    comet_scores2008.append((model_name, comet_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref1 COMET score (2009 Dataset): 0.7343478642448517\n"
     ]
    }
   ],
   "source": [
    "for model, comet_score in comet_scores2008:\n",
    "    print(f\"{model} COMET score (2008 Dataset): {comet_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2009 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e84eac44114c2db5c309bded63abac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/371e9839ca4e213dde891b066cf3080f75ec7e72/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/Users/ericbennett/miniforge3/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ericbennett/miniforge3/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Predicting DataLoader 0: 100%|████████████████| 126/126 [02:27<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "comet_scores2009 = []\n",
    "\n",
    "for model_name in translation_models:\n",
    "    formatted_data2009 = []\n",
    "    \n",
    "    for _, row in df2009.iterrows():\n",
    "        entry = {\n",
    "            \"src\": row['src'],\n",
    "            \"mt\": row[model_name],\n",
    "            \"ref\": [row['ref1'],row['ref2'], row['ref3']]\n",
    "        }\n",
    "        formatted_data2009.append(entry) \n",
    "    \n",
    "    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "    model = load_from_checkpoint(model_path)\n",
    "    model_output = model.predict(formatted_data, batch_size=8, gpus=0)\n",
    "    \n",
    "    \n",
    "    comet_score = sum(model_output.scores) / len(model_output.scores)\n",
    "    \n",
    "    comet_scores2009.append((model_name, comet_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref1 COMET score (2009 Dataset): 0.7343478642448517\n"
     ]
    }
   ],
   "source": [
    "for model_name, comet_score in comet_scores2009:\n",
    "    print(f\"{model_name} COMET score (2009 Dataset): {comet_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
